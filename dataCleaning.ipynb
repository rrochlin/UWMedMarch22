{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = glob.glob(\"./Data/*.txt\")\n",
    "# insert the desired start time\n",
    "cutOffTime = '3/22/2021 9:30'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01     2021-03-22 09:30:00      2021-03-22 13:00:19\nS-02     2021-03-22 09:30:01      2021-03-22 12:59:21\nS-03     2021-03-22 09:30:00      2021-03-22 12:59:09\nS-04     2021-03-22 09:30:00      2021-03-22 12:59:00\nS-05     2021-03-22 09:30:00      2021-03-22 12:59:29\nS-06     2021-03-22 09:30:00      2021-03-22 13:00:49\nS-07     2021-03-22 09:30:05      2021-03-22 12:58:50\nS-08     2021-03-22 09:30:05      2021-03-22 12:58:46\nS-09     2021-03-22 09:30:16      2021-03-22 12:59:26\nS-10     2021-03-22 09:30:06      2021-03-22 12:59:32\nS-11     2021-03-22 09:30:03      2021-03-22 12:59:53\nS-12     2021-03-22 09:30:08      2021-03-22 12:59:30\nS-13     2021-03-22 09:30:05      2021-03-22 12:59:15\nS-15     2021-03-22 09:30:00      2021-03-22 12:59:03\nS-16     2021-03-22 09:30:00      2021-03-22 13:32:35\nS-17     2021-03-22 09:30:00      2021-03-22 13:26:45\nS-18     2021-03-22 11:23:18      2021-03-22 13:01:22\nS-19     2021-03-22 09:30:07      2021-03-22 13:01:21\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns)"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.08 % potential error in  S-01\n",
      "  19\n",
      "   1\n",
      "\n",
      "0.0 % potential error in  S-02\n",
      "\n",
      "\n",
      "\n",
      "0.16 % potential error in  S-03\n",
      "  19  20\n",
      "   1   1\n",
      "\n",
      "0.24 % potential error in  S-04\n",
      "  21   9  20\n",
      "   1   1   1\n",
      "\n",
      "1.02 % potential error in  S-05\n",
      "  19 850  11   9\n",
      "   1   1   5   5\n",
      "\n",
      "0.08 % potential error in  S-06\n",
      "   9\n",
      "   1\n",
      "\n",
      "20.02 % potential error in  S-07\n",
      "  26  19  20\n",
      "   1   1 207\n",
      "\n",
      "20.31 % potential error in  S-08\n",
      "  21  11  20  19\n",
      "   1   3 205   3\n",
      "\n",
      "49.88 % potential error in  S-09\n",
      "  20\n",
      " 418\n",
      "\n",
      "26.97 % potential error in  S-10\n",
      "  26 130   9  19   3  20   4  27  11\n",
      "  22   1   3   2  43 145  16  38  10\n",
      "\n",
      "20.0 % potential error in  S-11\n",
      "  20\n",
      " 210\n",
      "\n",
      "99.68 % potential error in  S-12\n",
      "  211319  19   0  20  11\n",
      "   6   1   4  61 551   1\n",
      "\n",
      "19.96 % potential error in  S-13\n",
      "  20\n",
      " 209\n",
      "\n",
      "0.64 % potential error in  S-15\n",
      "  38  14  12  24  51   0  13  11\n",
      "   1   1   1   1   1   1   1   1\n",
      "\n",
      "0.28 % potential error in  S-16\n",
      "  14  66   1\n",
      "   2   1   1\n",
      "\n",
      "0.21 % potential error in  S-17\n",
      "  14   9  12\n",
      "   1   1   1\n",
      "\n",
      "0.17 % potential error in  S-18\n",
      "  14\n",
      "   1\n",
      "\n",
      "0.08 % potential error in  S-19\n",
      "  14\n",
      "   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "errors = {}\n",
    "errorCount = {}\n",
    "# Enter the expected interval here\n",
    "interval = 10\n",
    "for x in data:\n",
    "    # errors keeps track of length of each time interval error that occurs\n",
    "    errors[x] = set(())\n",
    "    # errorCount keeps track of how many times each time interval error occured\n",
    "    errorCount[x] = {}\n",
    "    # counter keeps track of the total time interval errors per sensor\n",
    "    counter = 0\n",
    "    #shows the total\n",
    "    temp = data[x]\n",
    "    for idx,i in enumerate(temp['Date_Time']):\n",
    "        try:\n",
    "            if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                timeErr = temp['Date_Time'][idx+1] - i\n",
    "                if str(timeErr.seconds) in errorCount[x]:\n",
    "                    errorCount[x][str(timeErr.seconds)] +=1\n",
    "                else:\n",
    "                    errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                counter += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "    fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "    # display the different types of errors\n",
    "    lst = [i.seconds for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    # display the quantity of each type of error\n",
    "    lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    print()\n",
    "    fout.write('\\n')\n",
    "\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-02   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-03   ['% of values from interpolation : 0.159', '% of values from 0-padding : 0.0', '% of values not changed : 99.841']\n",
      "S-04   ['% of values from interpolation : 0.398', '% of values from 0-padding : 0.0', '% of values not changed : 99.602']\n",
      "S-05   ['% of values from interpolation : 0.477', '% of values from 0-padding : 6.762', '% of values not changed : 92.761']\n",
      "S-06   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-07   ['% of values from interpolation : 33.413', '% of values from 0-padding : 0.0', '% of values not changed : 66.587']\n",
      "S-08   ['% of values from interpolation : 33.36', '% of values from 0-padding : 0.0', '% of values not changed : 66.64']\n",
      "S-09   ['% of values from interpolation : 66.667', '% of values from 0-padding : 0.0', '% of values not changed : 33.333']\n",
      "S-10   ['% of values from interpolation : 36.407', '% of values from 0-padding : 1.033', '% of values not changed : 62.56']\n",
      "S-11   ['% of values from interpolation : 33.333', '% of values from 0-padding : 0.0', '% of values not changed : 66.667']\n",
      "S-12   ['% of values from interpolation : 89.348', '% of values from 0-padding : 10.413', '% of values not changed : 0.238']\n",
      "S-13   ['% of values from interpolation : 33.28', '% of values from 0-padding : 0.0', '% of values not changed : 66.72']\n",
      "S-15   ['% of values from interpolation : 0.637', '% of values from 0-padding : 0.398', '% of values not changed : 98.964']\n",
      "S-16   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.556', '% of values not changed : 99.444']\n",
      "S-17   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-18   ['% of values from interpolation : 0.159', '% of values from 0-padding : 53.968', '% of values not changed : 45.873']\n",
      "S-19   ['% of values from interpolation : 0.159', '% of values from 0-padding : 0.0', '% of values not changed : 99.841']\n"
     ]
    }
   ],
   "source": [
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,'3/22/2021 9:30','3/22/2021 13:00',cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./interpolatedData\\S-19.csv\n"
     ]
    }
   ],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames\n",
    "Also remove 'S-02' from the dictionary as it has no real data\n",
    "and find the least common index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1253\n"
     ]
    }
   ],
   "source": [
    "# interpDF.pop('S-02',None)\n",
    "# interpDF.pop('S-BU2',None)\n",
    "# interpDF.pop('S-BU1',None)\n",
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "tempList = temp[15:19]\n",
    "tempList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 S-01 0\n2 S-02 0\n3 S-03 9\n4 S-04 0\n5 S-05 0\n6 S-06 0\n7 S-07 0\n8 S-08 4\n9 S-09 0\n10 S-10 0\n11 S-11 0\n12 S-12 0\n13 S-13 0\n14 S-15 0\n15 S-16 0\n16 S-17 0\n17 S-18 0\n18 S-19 0\n"
     ]
    }
   ],
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMerged = []\n",
    "columns = list(interpDF.keys())\n",
    "columns.extend(['Average',\n",
    "'Variance',\n",
    "'Zone 1',\n",
    "'Var Z1',\n",
    "'Zone 2',\n",
    "'Var Z2',\n",
    "'Zone 3',\n",
    "'Var Z3'])\n",
    "# 'Zone 4',\n",
    "# 'Var Z4'])\n",
    "\n",
    "for idx,i in enumerate(interpDF[columns[0]].values[:index]):\n",
    "    temp = []\n",
    "    temp.append(i[0])\n",
    "    for x in interpDF:\n",
    "        temp.append(interpDF[x].values[idx][1])\n",
    "    #So we now have a list with the timestamp and then sensors\n",
    "    \n",
    "    #here we add the overall average and variance columns\n",
    "    temp.append(np.average(temp[1:16]))\n",
    "    temp.append(np.std(temp[1:16]))\n",
    "\n",
    "    #here we're segregating the zones in the file giving their variance and avg\n",
    "\n",
    "    #Zone 1 the 2 sensors right on top of the nebulizer\n",
    "    lst = temp[1:7]\n",
    "    temp.append(np.average(lst))\n",
    "    temp.append(np.std(lst))\n",
    "    # #Zone 2 the perimiter of the bed\n",
    "    # lst = [temp[2],temp[3],temp[5],temp[6]]\n",
    "    # temp.append(np.average(lst))\n",
    "    # temp.append(np.std(lst))\n",
    "    #Zone 3 the perimeter of the room\n",
    "    lst = temp[7:16]\n",
    "    temp.append(np.average(lst))\n",
    "    temp.append(np.std(lst))\n",
    "    #Zone 4 is just the outside sensor\n",
    "    lst = temp[16:19]\n",
    "    temp.append(np.average(lst))\n",
    "    temp.append(np.std(lst))\n",
    "    dfMerged.append(temp)\n",
    "columns.insert(0,'Date_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = pd.DataFrame(dfMerged,columns = columns)"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "expTRange = {\n",
    "\n",
    "    'EE502 Door Closed':\n",
    "    [pd.Timestamp('3/22/2021 9:40'),\n",
    "    pd.Timestamp('3/22/2021 10:05:23'),\n",
    "    pd.Timestamp('3/22/2021 10:23:52')],\n",
    "    'EE502 Door Open':\n",
    "    [pd.Timestamp('3/22/2021 10:42:43'),\n",
    "    pd.Timestamp('3/22/2021 10:59:23'),\n",
    "    pd.Timestamp('3/22/2021 11:15:37')],\n",
    "    'EE502 Negative Pressure':\n",
    "    [pd.Timestamp('3/22/2021 11:32:21'),\n",
    "    pd.Timestamp('3/22/2021 11:42:27'),\n",
    "    pd.Timestamp('3/22/2021 11:53:47')],\n",
    "    'EE504 Door Open':\n",
    "    [pd.Timestamp('3/22/2021 12:19:12'),\n",
    "    pd.Timestamp('3/22/2021 12:30:10'),\n",
    "    pd.Timestamp('3/22/2021 12:40:15')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'EE502 Door Closed' : 15*6,\n",
    "    'EE502 Door Open':15*6,\n",
    "    'EE502 Negative Pressure':10*6,\n",
    "    'EE504 Door Open':10*6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "metadata": {},
     "execution_count": 268
    }
   ],
   "source": [
    "expTLen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls how many seconds of data before each experiment to include\n",
    "preCursorFactor = 6\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "\n",
    "    df1Index1 = expIndexes[label][0] - preCursorFactor\n",
    "    df1Index2 = expIndexes[label][0] + expTLen[label]\n",
    "    df1 = mergedData.iloc[df1Index1 : df1Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    df2Index1 = expIndexes[label][1] - preCursorFactor\n",
    "    df2Index2 = expIndexes[label][1] + expTLen[label]\n",
    "    df2 = mergedData.iloc[df2Index1 : df2Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    df3Index1 = expIndexes[label][2] - preCursorFactor\n",
    "    df3Index2 = expIndexes[label][2] + expTLen[label]\n",
    "    df3 = mergedData.iloc[df3Index1 : df3Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    averagedFrame[label] = (df1 + df2 + df3)/3\n",
    "\n",
    "    expirementFrame[label+' Exp1'] = df1\n",
    "    expirementFrame[label+' Exp2'] = df2\n",
    "    expirementFrame[label+' Exp3'] = df3\n",
    "    \n",
    "#assuming there were 3 expirements for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  }
 ]
}